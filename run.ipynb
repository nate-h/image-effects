{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4f0786a",
   "metadata": {},
   "source": [
    "# Image Effects\n",
    "\n",
    "This notebook contains a couple dozen image transformations.\n",
    "\n",
    "Some recreating trivial photoshop effects.\n",
    "\n",
    "Others just having fun with opencv :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import math\n",
    "import textwrap\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from src.utils import load_rgb, adjust_exponential, plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "318e9f20",
   "metadata": {},
   "source": [
    "## Color dot\n",
    "\n",
    "- Choose dot size\n",
    "- Create down sampled copy of original image using dot size\n",
    "- Create color_dot image matching the original images dimensions\n",
    "- Use nested for-loop to add circles to color_dot using the down sampled image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca68904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and create down sampled copy.\n",
    "rgb = load_rgb(\"img/face.jpg\")\n",
    "dot_size = 15\n",
    "small_shape = (rgb.shape[1] // dot_size, rgb.shape[0] // dot_size)\n",
    "down_sampled = cv2.resize(rgb, small_shape)\n",
    "\n",
    "# Draw circles.\n",
    "radius = dot_size // 2\n",
    "color_dot = np.zeros(rgb.shape, dtype=np.uint8)\n",
    "for i in range(down_sampled.shape[1]):\n",
    "    for j in range(down_sampled.shape[0]):\n",
    "        color = down_sampled[j, i, :].tolist()\n",
    "        center = (dot_size * i + radius), (dot_size * j + radius)\n",
    "        cv2.circle(color_dot, center, radius, color, -1)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Down Sampled\", down_sampled),\n",
    "    (\"Color Dot\", color_dot),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "318e9f20",
   "metadata": {},
   "source": [
    "## Halftone\n",
    "\n",
    "The approach is very similar to color dot.\n",
    "\n",
    "- Choose halftone circle size and call that block size\n",
    "- Create down sampled copy of original image using block size\n",
    "- Compute gray scale of down sampled copy\n",
    "- Create halftone image matching the original images dimensions\n",
    "- Use nested for-loop to add circles to halftone using the grayscale pixel intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca68904",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/face.jpg\")\n",
    "block_size = 10\n",
    "small_shape = (rgb.shape[1] // block_size, rgb.shape[0] // block_size)\n",
    "down_sampled = cv2.resize(rgb, small_shape)\n",
    "gray = cv2.cvtColor(down_sampled, cv2.COLOR_RGB2GRAY) / 255\n",
    "radius = block_size // 2\n",
    "\n",
    "# Draw circles.\n",
    "halftone = np.full(rgb.shape[0:2], 255, dtype=np.uint8)\n",
    "for i in range(gray.shape[1]):\n",
    "    for j in range(gray.shape[0]):\n",
    "        value = 1 - gray[j, i]\n",
    "        center = (block_size * i + radius), (block_size * j + radius)\n",
    "        cv2.circle(halftone, center, int(value * radius), 0, -1)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Down Sampled Gray\", gray),\n",
    "    (\"Halftone\", halftone),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cf47780",
   "metadata": {},
   "source": [
    "## Fit arc to ball trajectory\n",
    "\n",
    "I couldn't find a good image online showing a real parabolic trajectory of a ball.\n",
    "This simulated image will do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67a5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/ball_trajectory.jpg\")\n",
    "smoothed_rgb = cv2.GaussianBlur(rgb, (11, 11), 0)\n",
    "hsv = cv2.cvtColor(smoothed_rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Threshold on ball color.\n",
    "lower = np.array((0, 60, 60), dtype=np.uint8)\n",
    "upper = np.array((20, 255, 255), dtype=np.uint8)\n",
    "threshold = cv2.inRange(hsv, lower, upper)\n",
    "dilated = cv2.dilate(threshold, np.ones((2, 2), np.uint8), iterations=4)\n",
    "\n",
    "# Find balls.\n",
    "points = []\n",
    "contours, _ = cv2.findContours(\n",
    "    dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    ")\n",
    "found = rgb.copy()\n",
    "for c in contours:\n",
    "    (x, y), radius = cv2.minEnclosingCircle(c)\n",
    "    center = (int(x), int(y))\n",
    "    radius = int(radius)\n",
    "    cv2.circle(found, center, radius, (0, 255, 0), -1)\n",
    "    points.append((int(x), int(y)))\n",
    "\n",
    "# Find arc of balls.\n",
    "points = sorted(points, key=lambda x: x[0])\n",
    "xs, ys = zip(*points)\n",
    "a, b, c = np.polyfit(xs, ys, 2)\n",
    "fit_xs = np.linspace(00, 250, 50)\n",
    "fit_ys = np.array([a * x ** 2 + b * x + c for x in fit_xs])\n",
    "fit_points = np.array(list(zip(fit_xs, fit_ys)), dtype=np.int64)\n",
    "arc = cv2.polylines(rgb.copy(), [fit_points], False, (255, 0, 255), 2)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    2,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Smoothed\", smoothed_rgb),\n",
    "    (\"Threshold\", threshold),\n",
    "    (\"Dilated\", dilated),\n",
    "    (\"Found\", found),\n",
    "    (\"Parabolic Fit\", arc),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae7ed1fb",
   "metadata": {},
   "source": [
    "## Cartoon Portrait\n",
    "\n",
    "The two main steps to creating a cartoon portrait are color quantizing and making\n",
    "prominent edges much darker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03306852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "rgb = load_rgb(\"img/sunglasses.jpg\")\n",
    "h, w = rgb.shape[0:2]\n",
    "\n",
    "# Reduce colors.\n",
    "n_colors = 7\n",
    "linear_rgb = rgb.reshape((-1, 3))\n",
    "kmeans = KMeans(n_clusters=n_colors, n_init=\"auto\").fit(linear_rgb)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "quantized = centers[labels].reshape(rgb.shape).astype(np.uint8)\n",
    "\n",
    "# Find edges.\n",
    "gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "smooth_gray = cv2.GaussianBlur(gray, (7, 7), 0)\n",
    "edges = cv2.Canny(image=smooth_gray, threshold1=160, threshold2=245)\n",
    "edges = 255 - cv2.dilate(edges, np.ones((2, 2)), iterations=1)\n",
    "\n",
    "# Work edges into quantized picture.\n",
    "bilateral = cv2.bilateralFilter(quantized, 9, 75, 75)\n",
    "cartoon = cv2.bitwise_and(bilateral, bilateral, mask=edges)\n",
    "\n",
    "# Make it a portrait.\n",
    "mask = np.full(cartoon.shape[0:2], 1, dtype=np.float64)\n",
    "mask = cv2.circle(mask, (w // 2, h // 2), h // 2, (0, 0, 0), -1)\n",
    "alpha = np.dstack([mask] * 3)\n",
    "portrait = (255 * alpha + (1 - alpha) * cartoon).astype(np.uint8)\n",
    "\n",
    "# Show original, pixelated, quantized_pixelated.\n",
    "plot(\n",
    "    2,\n",
    "    4,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Quantized\", quantized),\n",
    "    (\"Smooth Gray\", smooth_gray),\n",
    "    (\"edges\", edges),\n",
    "    (\"Bilateral\", bilateral),\n",
    "    (\"Cartoon\", cartoon),\n",
    "    (\"Mask\", mask),\n",
    "    (\"Portrait\", portrait),\n",
    "    figsize=(10, 4),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "799dd2b1",
   "metadata": {},
   "source": [
    "## Finding the horizon using a canny edge detector\n",
    "\n",
    "For this demo, I wanted to explore using a canny edge detector to separate the\n",
    "ground from the sky. Took a bit of work to find the right settings for the gaussian\n",
    "filter and canny hysteresis threshold.\n",
    "\n",
    "The gaussian filter is used to weed out weak edges in order to isolate the horizon.\n",
    "\n",
    "The [hysteresis threshold](https://theailearner.com/tag/hysteresis-thresholding/)\n",
    "controls how the edge detector connects weak edges to strong edges.\n",
    "\n",
    "Look at how `Canny` compares to `Canny Gaussian` to see why it's important to\n",
    "smooth the minor edges out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0df573",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/sunset.jpg\")\n",
    "gaussian = cv2.GaussianBlur(rgb, (21, 21), 0)\n",
    "\n",
    "# Explore canny edge detectors w/ and w/o smoothing.\n",
    "t1 = 100\n",
    "t2 = 240\n",
    "canny_rgb = cv2.Canny(image=rgb, threshold1=t1, threshold2=t2)\n",
    "canny_gaussian = cv2.Canny(image=gaussian, threshold1=t1, threshold2=t2)\n",
    "\n",
    "# Create new image.\n",
    "separated = np.zeros(rgb.shape, dtype=np.uint8)\n",
    "top_color = (255, 0, 0)\n",
    "middle_color = (255, 255, 255)\n",
    "bottom_color = (0, 0, 255)\n",
    "thickness = 30\n",
    "for col_index, col in enumerate(canny_gaussian.T):\n",
    "    sep = int(np.where(col == 255)[0].mean())\n",
    "    separated[: sep - thickness, col_index, :] = top_color\n",
    "    separated[sep - thickness : sep + thickness, col_index, :] = middle_color\n",
    "    separated[sep + thickness + 1 :, col_index, :] = bottom_color\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    5,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Gaussian\", gaussian),\n",
    "    (\"Canny\", canny_rgb),\n",
    "    (\"Canny Gaussian\", canny_gaussian),\n",
    "    (\"Horizon\", separated),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83997570",
   "metadata": {},
   "source": [
    "## Median vs Gaussian blurs on noise reduction\n",
    "\n",
    "Both median and gaussian blur operations smooth out images but have different\n",
    "effects. Median blurring works by looking at every cell and taking the median of\n",
    "it and all neighboring cells. Gaussian blurring works by looking at every cell and\n",
    "averaging it with its neighboring cells.\n",
    "\n",
    "One benefit of median blurring is its good at getting rid of noise instead of\n",
    "averaging it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee51bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/wink.png\")\n",
    "noise = np.random.choice([0, 255], size=rgb.shape, p=[0.9, 0.1])\n",
    "noisy_rgb = (noise + rgb).astype(np.uint8)\n",
    "\n",
    "plot(\n",
    "    2,\n",
    "    2,\n",
    "    (\"Original\", rgb),\n",
    "    (\"With noise\", noisy_rgb),\n",
    "    (\"Median\", cv2.medianBlur(noisy_rgb, 5)),\n",
    "    (\"Gaussian\", cv2.GaussianBlur(noisy_rgb, (5, 5), 0)),\n",
    "    figsize=(4, 4),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ed0ec54",
   "metadata": {},
   "source": [
    "## Find tennis ball\n",
    "\n",
    "\n",
    "To find the tennis ball:\n",
    "\n",
    "- Look for it's approximate colors\n",
    "- Try to filter out unwanted colors\n",
    "- Find the contour around the blobs found in the mask\n",
    "- Find the minimum circle around the biggest blob and consider it the ball\n",
    "\n",
    "Note: the values for the threshold we're hand tuned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202136f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/tennis_ball.jpg\")\n",
    "smoothed_rgb = cv2.GaussianBlur(rgb, (11, 11), 0)\n",
    "hsv = cv2.cvtColor(smoothed_rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Find pixels likely belonging to tennis ball.\n",
    "lower = np.array((34, 152, 132), dtype=np.uint8)\n",
    "upper = np.array((44, 255, 255), dtype=np.uint8)\n",
    "threshold = cv2.inRange(hsv, lower, upper)\n",
    "eroded = cv2.erode(threshold, np.ones((5, 5), np.uint8), iterations=3)\n",
    "dilated = cv2.dilate(eroded, np.ones((5, 5), np.uint8), iterations=4)\n",
    "\n",
    "# Find tennis ball from probable tennis ball pixels.\n",
    "contours, _ = cv2.findContours(\n",
    "    dilated.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    ")\n",
    "rgb_copy = rgb.copy()\n",
    "cv2.drawContours(rgb_copy, contours, -1, 255, 3)\n",
    "(x, y), radius = cv2.minEnclosingCircle(contours[0])\n",
    "center = (int(x), int(y))\n",
    "radius = int(radius)\n",
    "cv2.circle(rgb_copy, center, radius, (0, 255, 0), 2)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    6,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Smoothed\", smoothed_rgb),\n",
    "    (\"Threshold\", threshold),\n",
    "    (\"Eroded\", eroded),\n",
    "    (\"Dilated\", dilated),\n",
    "    (\"Found\", rgb_copy),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b3eee5b",
   "metadata": {},
   "source": [
    "## Grayscale\n",
    "\n",
    "One might think grayscale is as easy as averaging the rgb channels but our eyes\n",
    "perceive things differently. A better way to grayscale is to use non-linear set\n",
    "of weights when combining the rgb channels.\n",
    "\n",
    "More here: https://en.wikipedia.org/wiki/Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b21adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/lenna.png\")\n",
    "gray = np.dot(rgb[..., :3], [0.299, 0.587, 0.114])\n",
    "open_cv_gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n",
    "plot(1, 3, (\"Original\", rgb), (\"Manual Gray\", gray), (\"OpenCV Gray\", open_cv_gray))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c390359",
   "metadata": {},
   "source": [
    "## Hue shifting\n",
    "\n",
    "Load image as hsv then shift all colors using a simple offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a424d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load colorful image.\n",
    "rgb = load_rgb(\"img/food.jpg\")\n",
    "hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Created shifted image.\n",
    "hue = hsv[..., 0]\n",
    "shift = 90\n",
    "shifted_hue = (hue + shift) % 180\n",
    "hsv[:, :, 0] = shifted_hue\n",
    "shifted_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "\n",
    "# Plot comparison.\n",
    "plot(1, 2, (\"Original\", rgb), (\"Shifted Hue\", shifted_rgb), figsize=(4, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac7c4783",
   "metadata": {},
   "source": [
    "## Modeling macos dynamic backgrounds\n",
    "\n",
    "Use [gamma correction](https://en.wikipedia.org/wiki/Gamma_correction) to\n",
    "emulate macos dynamic backgrounds.\n",
    "Works non-linearly scaling each channel in rgb space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/beach.jpg\")\n",
    "gammas = [1.3, 1, 0.7, 0.4]\n",
    "\n",
    "divides = np.linspace(0, rgb.shape[1], 5).astype(int)\n",
    "for start, stop, gamma in zip(divides, divides[1:], gammas):\n",
    "    rgb[:, start:stop, :] = adjust_exponential(rgb[:, start:stop, :], gamma)\n",
    "\n",
    "plot(1, 1, (\"Modeling macos dynamic backgrounds\", rgb), figsize=(4, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c23543",
   "metadata": {},
   "source": [
    "## Shifting saturation\n",
    "\n",
    "Same concept as previous demo but adjusting saturation in hsv space instead\n",
    "of each channel in rgb space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/beach.jpg\")\n",
    "hsv_exp = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "hsv_lin = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "saturations = [0.4, 0.7, 1, 1.3]\n",
    "divides = np.linspace(0, hsv_exp.shape[1], len(saturations) + 1).astype(int)\n",
    "\n",
    "for start, stop, saturation in zip(divides, divides[1:], saturations):\n",
    "    hsv_lin[:, start:stop, 1] = hsv_lin[:, start:stop, 1] * saturation\n",
    "    hsv_exp[:, start:stop, 1] = adjust_exponential(\n",
    "        hsv_exp[:, start:stop, 1], saturation\n",
    "    )\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Linear\", cv2.cvtColor(hsv_lin, cv2.COLOR_HSV2RGB)),\n",
    "    (\"Exponential\", cv2.cvtColor(hsv_exp, cv2.COLOR_HSV2RGB)),\n",
    "    figsize=(8, 8),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f20dd37",
   "metadata": {},
   "source": [
    "## Color quantization + pixelation\n",
    "\n",
    "Use color quantization + pixelation to attempt to convert new mario photo into\n",
    "old 1980s version.\n",
    "Color quantization works by using\n",
    "[k-keans](https://en.wikipedia.org/wiki/K-means_clustering) clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243f42b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "rgb = load_rgb(\"img/mario.png\")\n",
    "\n",
    "# Create pixelated image.\n",
    "orig_h, orig_w = rgb.shape[:2]\n",
    "pixel_w, pixel_h = (25, 38)\n",
    "down_sampled = cv2.resize(rgb, (pixel_w, pixel_h), interpolation=cv2.INTER_LINEAR)\n",
    "pixelated = cv2.resize(down_sampled, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Quantize the pixelated image.\n",
    "n_colors = 7\n",
    "arr = pixelated.reshape((-1, 3))\n",
    "kmeans = KMeans(n_clusters=n_colors, n_init=\"auto\").fit(arr)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "quantized_pixelated = centers[labels].reshape(pixelated.shape).astype(np.uint8)\n",
    "\n",
    "# Show original, pixelated, quantized_pixelated.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Pixelated\", pixelated),\n",
    "    (\"Quantized\", quantized_pixelated),\n",
    "    figsize=(4, 4),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731a5cb3",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e951a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/high-five.png\")\n",
    "reflected = cv2.flip(rgb, 1)\n",
    "joined = np.concatenate((rgb, reflected), axis=1)\n",
    "\n",
    "plot(1, 1, (\"Original + Reflected\", joined), figsize=(3, 3))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6180887",
   "metadata": {},
   "source": [
    "## Erode and Dilate to fill in holes\n",
    "\n",
    "This demo will load an image of a dice, find the dice, then fill in the holes\n",
    "by enlarging and shrinking the mask of the dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36696eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dice and convert to hsv.\n",
    "rgb = load_rgb(\"img/dice.png\")\n",
    "hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "# Select all the red to find the dice.\n",
    "lower = np.array([160, 0, 0], dtype=np.uint8)\n",
    "upper = np.array([185, 255, 255], dtype=np.uint8)\n",
    "mask = cv2.inRange(hsv, lower, upper)\n",
    "\n",
    "# Fill in the holes by enlarging then shrinking the mask.\n",
    "kernel = np.ones((20, 20), np.uint8)\n",
    "dilated = cv2.dilate(mask, kernel, iterations=1)\n",
    "eroded = cv2.erode(dilated, kernel, iterations=1)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    2,\n",
    "    2,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Mask\", mask),\n",
    "    (\"Dilated\", dilated),\n",
    "    (\"Eroded\", eroded),\n",
    "    figsize=(3, 3),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfd1aca2",
   "metadata": {},
   "source": [
    "## Torn paper effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b397b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate torn paper mask from its alpha channel.\n",
    "bgra = cv2.imread(\"img/torn_paper.png\", flags=cv2.IMREAD_UNCHANGED)\n",
    "rgba = cv2.cvtColor(bgra, cv2.COLOR_BGRA2RGBA)\n",
    "rgba_rotated = cv2.rotate(rgba, cv2.ROTATE_90_CLOCKWISE)\n",
    "alpha = rgba_rotated[..., 3] / 255\n",
    "alpha = np.dstack([alpha] * 3)\n",
    "torn_paper = rgba_rotated[..., 0:3]\n",
    "\n",
    "# Load portrait.\n",
    "portrait = load_rgb(\"img/fashion_portrait.jpg\")\n",
    "portrait = cv2.resize(portrait, torn_paper.shape[:-1], interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "# Merge torn paper with portrait.\n",
    "merged = (alpha * torn_paper + (1 - alpha) * portrait).astype(np.uint8)\n",
    "\n",
    "# Display.\n",
    "plot(1, 3, (\"Mask\", rgba), (\"Portrait\", portrait), (\"Merged\", merged))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4712844",
   "metadata": {},
   "source": [
    "## Rotate a caret to create star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgra = cv2.imread(\"img/star_part.png\", flags=cv2.IMREAD_UNCHANGED)\n",
    "rgba = cv2.cvtColor(bgra, cv2.COLOR_BGR2RGBA)\n",
    "\n",
    "image_center = tuple(np.array(rgba.shape[1::-1]) / 2)\n",
    "\n",
    "star = rgba.copy()\n",
    "for i in range(1, 5):\n",
    "    rot_mat = cv2.getRotationMatrix2D(image_center, 72 * i, 1.0)\n",
    "    rotated = cv2.warpAffine(rgba, rot_mat, rgba.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    star += rotated\n",
    "\n",
    "plot(1, 2, (\"Original\", rgba), (\"5 rotations merged\", star), figsize=(5, 5))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b085683b",
   "metadata": {},
   "source": [
    "## Shifting portions of an image around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85c1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/pineapple.jpg\")\n",
    "shifted = np.zeros(rgb.shape, dtype=np.uint8)\n",
    "\n",
    "shift_scalar = 40\n",
    "height, width = rgb.shape[:-1]\n",
    "for i in range(height):\n",
    "    delta_x = int(shift_scalar * math.sin(math.radians(2 * i)))\n",
    "    insert_x = max(0, delta_x)\n",
    "    insert_x2 = min(width, width + delta_x)\n",
    "    extract_x1 = max(0, -delta_x)\n",
    "    extract_x2 = min(width, width - delta_x)\n",
    "    shifted[i, insert_x:insert_x2] = rgb[i, extract_x1:extract_x2]\n",
    "\n",
    "plot(1, 2, (\"Original\", rgb), (\"Shifted\", shifted), figsize=(4, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80914bc2",
   "metadata": {},
   "source": [
    "## Labeled shapes\n",
    "\n",
    "When using open cv's\n",
    "[findContours](https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html),\n",
    "it will usually produce a lot more edges than you would think. You can approximate\n",
    "that contour to get back to the expected amount of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012ecb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "rgb = load_rgb(\"img/shapes.jpg\")\n",
    "h, w = rgb.shape[0:2]\n",
    "\n",
    "# Find contours.\n",
    "_, thresh = cv2.threshold(rgb, 127, 255, cv2.THRESH_BINARY)\n",
    "contours, _ = cv2.findContours(\n",
    "    255 - thresh[..., 2], cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    ")\n",
    "shapes = thresh.copy()\n",
    "cv2.drawContours(shapes, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "# Create approximate contours.\n",
    "new_contours = []\n",
    "for c in contours:\n",
    "    approx_con = cv2.approxPolyDP(c, 0.01 * cv2.arcLength(c, True), True)\n",
    "    new_contours.append(approx_con)\n",
    "    approx_size = len(approx_con)\n",
    "    if approx_size == 3:\n",
    "        text = \"Triangle\"\n",
    "    elif approx_size == 4:\n",
    "        text = \" Square \"\n",
    "    else:\n",
    "        text = \" Circle \"\n",
    "    m = cv2.moments(c)\n",
    "    cx = int(m[\"m10\"] / m[\"m00\"])\n",
    "    cy = int(m[\"m01\"] / m[\"m00\"])\n",
    "    cv2.circle(shapes, (cx, cy), 3, (255, 0, 0), -1)\n",
    "    cv2.putText(\n",
    "        shapes, text, (cx - 45, cy - 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2\n",
    "    )\n",
    "\n",
    "# Draw approximate contours.\n",
    "cv2.drawContours(shapes, new_contours, -1, (0, 0, 255), 2)\n",
    "\n",
    "# Show original, pixelated, quantized_pixelated.\n",
    "plot(\n",
    "    1,\n",
    "    2,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Labeled\", shapes),\n",
    "    figsize=(5, 3),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a54256",
   "metadata": {},
   "source": [
    "## Noise gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7423c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/pineapple.jpg\")\n",
    "noise_gradient = np.zeros(rgb.shape, dtype=np.uint8)\n",
    "\n",
    "noise_scalar = 0.5\n",
    "height, width = rgb.shape[:-1]\n",
    "for i in range(height):\n",
    "    noise = np.random.normal(0, i * noise_scalar, (width, 3))\n",
    "    noise_gradient[i] = (rgb[i] + noise).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "plot(1, 2, (\"Original\", rgb), (\"Noise Gradient\", noise_gradient), figsize=(4, 4))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "96a513f2",
   "metadata": {},
   "source": [
    "## Bloom effect\n",
    "\n",
    "Add a halo in an image and make it glow\n",
    "\n",
    "- Create halo in separate mask and blur it\n",
    "- Blend halo back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/person.png\")\n",
    "\n",
    "# Create halo and blur it.\n",
    "halo = np.zeros(shape=rgb.shape, dtype=np.uint8)\n",
    "center = (halo.shape[0] // 2, 9)\n",
    "halo_size = (40, 7)\n",
    "color = (255, 223, 50)\n",
    "halo = cv2.ellipse(halo, center, halo_size, 0, 0, 360, color, 2)\n",
    "halo = cv2.GaussianBlur(halo, (5, 5), sigmaX=3)\n",
    "\n",
    "# Blend the layers together.\n",
    "gain = 2\n",
    "overlaid_halo = cv2.addWeighted(rgb, 1, halo, gain, 0)\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1, 3, (\"Original\", rgb), (\"Halo\", halo), (\"Overlaid\", overlaid_halo), figsize=(4, 4)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b728c989",
   "metadata": {},
   "source": [
    "## Additive color mixing\n",
    "\n",
    "Tvs on a per pixel basis are actually made of three separate sub-pixels.\n",
    "The sub-pixels are comprising of red, green, blue.\n",
    "\n",
    "I'm curious what recreating this effect looks like if I make an image pixel\n",
    "the subpixel and consider every three rows a pixel.\n",
    "\n",
    "I explore 2 different methods here:\n",
    "\n",
    "1. Make every row either red, green or blue depending on row mod 3x\n",
    "2. Resize by a factor of 3 then merge\n",
    "\n",
    "The result is lame lol. I'm essentially just throwing away 2/3 of the data but\n",
    "it was fun to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa81b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/parrot.jpg\")\n",
    "r, g, b = cv2.split(rgb)\n",
    "\n",
    "# Method 1: zeroing out channels.\n",
    "r[0::3] = 0\n",
    "r[1::3] = 0\n",
    "g[1::3] = 0\n",
    "g[2::3] = 0\n",
    "b[2::3] = 0\n",
    "b[0::3] = 0\n",
    "method_1 = cv2.merge((r, g, b))\n",
    "\n",
    "# Method 2: downsample then upsample selectively.\n",
    "h, w = rgb.shape[:-1]\n",
    "down_sampled_rgb = cv2.resize(rgb, (w, h // 3))\n",
    "down_sampled_r, down_sampled_g, down_sampled_b = cv2.split(down_sampled_rgb)\n",
    "method_2 = np.zeros(rgb.shape, dtype=rgb.dtype)\n",
    "method_2[0::3, :, 0] = down_sampled_r\n",
    "method_2[1::3, :, 1] = down_sampled_g\n",
    "method_2[2::3, :, 2] = down_sampled_b\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Method 1\", method_1),\n",
    "    (\"Method 2\", method_2),\n",
    "    figsize=(5, 5),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e950b826",
   "metadata": {},
   "source": [
    "## Cut out text overlay effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe180ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = load_rgb(\"img/fashion_portrait.jpg\")\n",
    "height, width = rgb.shape[:-1]\n",
    "\n",
    "\n",
    "def create_text_mask(fontFace, fontScale):\n",
    "    lorem_ipsum = (\n",
    "        \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, \"\n",
    "        \"sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. \"\n",
    "        \"Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris \"\n",
    "        \"nisi ut aliquip ex ea commodo consequat.\"\n",
    "    )\n",
    "    wrapped_text = textwrap.wrap(lorem_ipsum, width=35)\n",
    "    text_mask = np.zeros(rgb.shape, np.uint8)\n",
    "    for i, line in enumerate(wrapped_text, start=1):\n",
    "        cv2.putText(\n",
    "            text_mask,\n",
    "            text=line,\n",
    "            org=(0, i * 40),\n",
    "            fontFace=fontFace,\n",
    "            fontScale=fontScale,\n",
    "            color=(255, 255, 255),\n",
    "            thickness=3,\n",
    "        )\n",
    "    return text_mask\n",
    "\n",
    "\n",
    "# Create text masks.\n",
    "text_mask_1 = create_text_mask(7, 3)\n",
    "text_mask_2 = create_text_mask(0, 1)\n",
    "\n",
    "# Rotate second text mask.\n",
    "image_center = tuple(np.array((height, width)) / 2)\n",
    "rot_mat = cv2.getRotationMatrix2D(image_center, 90, 1.0)\n",
    "text_mask_2 = cv2.warpAffine(\n",
    "    text_mask_2, rot_mat, (height, width), flags=cv2.INTER_LINEAR\n",
    ")\n",
    "\n",
    "# Combine text masks then use that as a mask for the rgb image.\n",
    "text_mask = cv2.bitwise_or(text_mask_1, text_mask_2)\n",
    "combined = cv2.bitwise_and(rgb, text_mask)\n",
    "\n",
    "# Display.\n",
    "plot(1, 3, (\"Original\", rgb), (\"Text\", text_mask), (\"Combined\", combined))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb187332",
   "metadata": {},
   "source": [
    "## RGB to CMY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a393ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rgb and break apart.\n",
    "rgb = load_rgb(\"img/parrot.jpg\")\n",
    "r, g, b = cv2.split(rgb)\n",
    "\n",
    "# Create cmy.\n",
    "zeros_chan = np.zeros(rgb.shape[:-1], dtype=np.uint8)\n",
    "c = np.dstack((zeros_chan, g // 2, b // 2))  # c = g/2 + b/2.\n",
    "m = np.dstack((r // 2, zeros_chan, b // 2))  # m = r/2 + b/2.\n",
    "y = np.dstack((r // 2, g // 2, zeros_chan))  # y = r/2 + g/2.\n",
    "\n",
    "# Display.\n",
    "plot(\n",
    "    1,\n",
    "    5,\n",
    "    (\"Original\", rgb),\n",
    "    (\"C\", c),\n",
    "    (\"M\", m),\n",
    "    (\"Y\", y),\n",
    "    (\"Reconstructed\", c + m + y),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c6ca831",
   "metadata": {},
   "source": [
    "## Concentric rings\n",
    "\n",
    "When working with unsigned int8s, adding 1 to 255 will make 0.\n",
    "This concept is more broadly called\n",
    "[integer overflow](https://en.wikipedia.org/wiki/Integer_overflow).\n",
    "\n",
    "The program below uses that to cheaply create this cool effect.\n",
    "It simply renders 2 rings to a mask then adds that to the original image.\n",
    "Any overflows will simply wrap without error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image.\n",
    "rgb = load_rgb(\"img/sunglasses.jpg\")\n",
    "h, w = rgb.shape[0:2]\n",
    "\n",
    "# Make rings.\n",
    "rings = np.zeros(rgb.shape, dtype=np.uint8)\n",
    "radii = [w, w // 2, h // 2]\n",
    "colors = [(50, 40, 165), (190, 150, 30), (0, 0, 0)]\n",
    "for r, c in zip(radii, colors):\n",
    "    rings = cv2.circle(rings, (w // 2, h // 2), r, c, -1)\n",
    "\n",
    "# Show original, pixelated, quantized_pixelated.\n",
    "plot(\n",
    "    1,\n",
    "    3,\n",
    "    (\"Original\", rgb),\n",
    "    (\"Rings\", rings),\n",
    "    (\"Portrait\", rgb + rings),\n",
    "    figsize=(8, 4),\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-effects",
   "language": "python",
   "name": "image-effects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
